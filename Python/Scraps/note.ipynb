{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An: 0\n",
      "Ly: 1\n",
      "Nam: 1\n",
      "Mai: 2\n",
      "Hoa: 0\n",
      "Linh: 2\n",
      "[2, 0, 0, 1, 1]\n",
      "[0, 0, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Linh', 'chorio', 'medium', 'thick'),\n",
       " ('Nam', 'pinaapple', 'small', 'thick'),\n",
       " ('Mai', 'cheese', 'small', 'thick'),\n",
       " ('Ly', 'beef', 'medium', 'thin'),\n",
       " ('Mai', 'chorio', 'small', 'thin'),\n",
       " ('Linh', 'beef', 'medium', 'thin'),\n",
       " 'No medium left, choose a different size, please!',\n",
       " 'No small left, choose a different size, please!',\n",
       " 'No chorio left, choose a different topping, please!',\n",
       " 'No chorio left, choose a different topping, please!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = input()\n",
    "\n",
    "sentence = sentence.split()\n",
    "\n",
    "max = 0\n",
    "longest = \"\"\n",
    "for words in sentence:\n",
    "    count = 0\n",
    "    for letters in words:\n",
    "        count += 1\n",
    "    if max < count:\n",
    "        max = count\n",
    "        longest = words\n",
    "\n",
    "print(longest, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = input()\n",
    "\n",
    "sentence = sentence.split()\n",
    "\n",
    "result = \"\"\n",
    "for words in sentence:\n",
    "    if words[0].isupper():\n",
    "        result += words[0]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = input()\n",
    "n = int(input())\n",
    "\n",
    "sentence = sentence.split()\n",
    "\n",
    "result = \"\"\n",
    "for words in sentence:\n",
    "    count = 0\n",
    "    for letters in words:\n",
    "        count += 1\n",
    "    if count >= n:\n",
    "        result += words + \"-\"\n",
    "\n",
    "result = result[:-1]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remove': 1,\n",
       " 'frame': 1,\n",
       " 'settings': 1,\n",
       " 'software': 1,\n",
       " 'fairly': 1,\n",
       " 'beings': 1,\n",
       " 'labor': 1,\n",
       " 'types': 1,\n",
       " 'fraudulent': 1,\n",
       " 'part': 1,\n",
       " 'car': 1,\n",
       " 'theoretical': 1,\n",
       " 'collect': 1,\n",
       " 'previous': 1,\n",
       " 'vitals': 1,\n",
       " 'chatgpt': 1,\n",
       " 'carry': 1,\n",
       " 'machinery': 1,\n",
       " 'reason': 1,\n",
       " 'make': 1,\n",
       " 'join': 1,\n",
       " 'skills': 1,\n",
       " 'states': 1,\n",
       " 'successfully': 1,\n",
       " '79': 1,\n",
       " 'dosages': 1,\n",
       " 'two': 1,\n",
       " 'assistants': 1,\n",
       " 'games': 1,\n",
       " 'intelligence\\\\': 1,\n",
       " 'weak': 1,\n",
       " 'small': 1,\n",
       " 'automatically': 1,\n",
       " 'communicate': 1,\n",
       " 'claims': 1,\n",
       " 'type': 1,\n",
       " 'becoming': 1,\n",
       " 'technology': 1,\n",
       " 'may': 1,\n",
       " 'simple': 1,\n",
       " 'set': 1,\n",
       " 'simulate': 1,\n",
       " 'surpass': 1,\n",
       " 'pushed': 1,\n",
       " 'easily': 1,\n",
       " 'help': 1,\n",
       " 'indices': 1,\n",
       " 'making': 1,\n",
       " 'tasks': 1,\n",
       " 'openai': 1,\n",
       " 'game': 1,\n",
       " 'machine': 1,\n",
       " 'solving': 1,\n",
       " 'portfolio': 1,\n",
       " 'artificial+': 1,\n",
       " 'outputs': 1,\n",
       " 'funds': 1,\n",
       " 'vision': 1,\n",
       " 'systems': 1,\n",
       " 'network': 1,\n",
       " 'ask': 1,\n",
       " 'identify': 1,\n",
       " 'pre': 1,\n",
       " 'considered': 1,\n",
       " 'earn': 1,\n",
       " 'entered': 1,\n",
       " 'self': 1,\n",
       " 'embedded': 1,\n",
       " 'expect': 1,\n",
       " 'playing': 1,\n",
       " 'popular': 1,\n",
       " 'complex': 1,\n",
       " 'human': 1,\n",
       " 'room': 1,\n",
       " 'outdated': 1,\n",
       " 'cars': 1,\n",
       " 'allows': 1,\n",
       " '2022': 1,\n",
       " 'achieve': 1,\n",
       " 'surgical': 1,\n",
       " 'possibilities': 1,\n",
       " 'driving': 1,\n",
       " 'sectors': 1,\n",
       " 'ones': 1,\n",
       " 'sensors': 1,\n",
       " 'narrow': 1,\n",
       " 'photos': 1,\n",
       " 'profits': 1,\n",
       " 'enables': 1,\n",
       " 'tends': 1,\n",
       " 'aiding': 1,\n",
       " 'example': 1,\n",
       " '500': 1,\n",
       " 'research': 1,\n",
       " 'financial': 1,\n",
       " 'insurance': 1,\n",
       " 'enable': 1,\n",
       " 'include': 1,\n",
       " 'rooms': 1,\n",
       " 'goal': 1,\n",
       " 'survey': 1,\n",
       " 'assist': 1,\n",
       " 'taxis': 1,\n",
       " 'strategy': 1,\n",
       " 'major': 1,\n",
       " 'abilities': 1,\n",
       " 'application': 1,\n",
       " 'one': 1,\n",
       " '“what': 1,\n",
       " 'trading': 1,\n",
       " 'trained': 1,\n",
       " 'things': 1,\n",
       " 'using': 1,\n",
       " 'trade': 1,\n",
       " 'e': 1,\n",
       " 'invest': 1,\n",
       " 'possess': 1,\n",
       " 'programs': 1,\n",
       " 'united': 1,\n",
       " 'dall': 1,\n",
       " 'brought': 1,\n",
       " 'processing': 1,\n",
       " 'takeaways': 1,\n",
       " 'connectivity': 1,\n",
       " 'capabilities': 1,\n",
       " 'subset': 1,\n",
       " 'take': 1,\n",
       " 'evolved': 1,\n",
       " 'imagine': 1,\n",
       " 'intelligence': 1,\n",
       " 'components': 1,\n",
       " 'like': 1,\n",
       " 'technologies': 1,\n",
       " 'chess': 1,\n",
       " 'nlp': 1,\n",
       " 'diversify': 1,\n",
       " 'patient': 1,\n",
       " 'vantage': 1,\n",
       " 'novel': 1,\n",
       " 'ml': 1,\n",
       " 'detects': 1,\n",
       " '80': 1,\n",
       " 'images': 1,\n",
       " 'strong': 1,\n",
       " 'ai': 1,\n",
       " 'image': 1,\n",
       " 'learn': 1,\n",
       " 'leaders': 1,\n",
       " 'yet': 1,\n",
       " 'designed': 1,\n",
       " 'devices': 1,\n",
       " 'operating': 1,\n",
       " 'symptoms': 1,\n",
       " 'benchmarks': 1,\n",
       " 'assistance': 1,\n",
       " 'guide': 1,\n",
       " 'nasdaq': 1,\n",
       " '”': 1,\n",
       " 'apple': 1,\n",
       " 'tool': 1,\n",
       " 'also': 1,\n",
       " 'ideal': 1,\n",
       " 'particular': 1,\n",
       " 'known': 1,\n",
       " 'reactive': 1,\n",
       " 'without': 1,\n",
       " 'computers': 1,\n",
       " 'inputs': 1,\n",
       " 'employment': 1,\n",
       " 'trader': 1,\n",
       " 'hospital': 1,\n",
       " 'best': 1,\n",
       " 'simulated': 1,\n",
       " 'advisor': 1,\n",
       " 'deloitte': 1,\n",
       " 'jobs': 1,\n",
       " 'ais': 1,\n",
       " 'mainstream': 1,\n",
       " 'optimize': 1,\n",
       " 'unable': 1,\n",
       " 'generative': 1,\n",
       " 'applied': 1,\n",
       " 'patients': 1,\n",
       " 'found': 1,\n",
       " 'key': 1,\n",
       " 'easier': 1,\n",
       " '\"artificial': 1,\n",
       " 'language': 1,\n",
       " 'answers': 1,\n",
       " 'internet': 1,\n",
       " 'classify': 1,\n",
       " 'strictly': 1,\n",
       " 'uses': 1,\n",
       " 'better': 1,\n",
       " 's&p': 1,\n",
       " 'cars&': 1,\n",
       " 'video': 1,\n",
       " 'cfds': 1,\n",
       " 'machines': 1,\n",
       " 'algorithms': 1,\n",
       " 'according': 1,\n",
       " 'applications': 1,\n",
       " 'action': 1,\n",
       " 'implementation': 1,\n",
       " 'think': 1,\n",
       " 'discussion': 1,\n",
       " 'affect': 1,\n",
       " 'deal': 1,\n",
       " '1950s': 1,\n",
       " 'natural': 1,\n",
       " 'cognitive': 1,\n",
       " 'mimic': 1,\n",
       " 'department': 1,\n",
       " 'often': 1,\n",
       " 'records': 1,\n",
       " 'transformer': 1,\n",
       " 'concept': 1,\n",
       " 'activity': 1,\n",
       " 'alexa': 1,\n",
       " 'including': 1,\n",
       " 'graphics': 1,\n",
       " 'training': 1,\n",
       " '\\\\applications': 1,\n",
       " 'concern': 1,\n",
       " 'concerned': 1,\n",
       " 'became': 1,\n",
       " '\"': 1,\n",
       " 'ability': 1,\n",
       " 'characteristic': 1,\n",
       " 'play': 1,\n",
       " 'manufacturers': 1,\n",
       " 'super': 1,\n",
       " 'reasoning': 1,\n",
       " 'healthcare': 1,\n",
       " 'medical': 1,\n",
       " 'works': 1,\n",
       " 'define': 1,\n",
       " 'chips': 1,\n",
       " 'realized': 1,\n",
       " 'transform': 1,\n",
       " 'anomalies': 1,\n",
       " 'form': 1,\n",
       " 'win': 1,\n",
       " 'elite': 1,\n",
       " 'industries': 1,\n",
       " 'download': 1,\n",
       " 'looking': 1,\n",
       " 'amazon': 1,\n",
       " 'based': 1,\n",
       " 'units': 1,\n",
       " 'employees': 1,\n",
       " 'upon': 1,\n",
       " 'problem': 1,\n",
       " 'identifying': 1,\n",
       " 'drug': 1,\n",
       " 'automate': 1,\n",
       " '000': 1,\n",
       " 'new': 1,\n",
       " 'graphical': 1,\n",
       " 'obsolete': 1,\n",
       " 'examples': 1,\n",
       " 'workforce': 1,\n",
       " 'health': 1,\n",
       " 'rationalize': 1,\n",
       " 'objects': 1,\n",
       " 'client': 1,\n",
       " 'defense': 1,\n",
       " '1960s': 1,\n",
       " 'intelligent': 1,\n",
       " 'suggesting': 1,\n",
       " 'diagnostics': 1,\n",
       " 'treatments': 1,\n",
       " 'adapt': 1,\n",
       " 'detect': 1,\n",
       " 'began': 1,\n",
       " '200': 1,\n",
       " 'mind': 1,\n",
       " 'pictures': 1,\n",
       " 'many': 1,\n",
       " 'diagnoses': 1,\n",
       " 'complicated': 1,\n",
       " 'commonly': 1,\n",
       " 'understand': 1,\n",
       " 'static': 1,\n",
       " 'users': 1,\n",
       " 'triangulate': 1,\n",
       " 'need': 1,\n",
       " 'people': 1,\n",
       " 'assistant': 1,\n",
       " 'mathematical': 1,\n",
       " 'siri': 1,\n",
       " 'computer': 1,\n",
       " 'track': 1,\n",
       " 'flags': 1,\n",
       " 'tend': 1,\n",
       " '2024': 1,\n",
       " 'includes': 1,\n",
       " 'personal': 1,\n",
       " 'maintain': 1,\n",
       " 'streamline': 1,\n",
       " 'vehicles': 1,\n",
       " 'share': 1,\n",
       " 'text': 1,\n",
       " 'general': 1,\n",
       " 'specific': 1,\n",
       " 'programming': 1,\n",
       " 'used': 1,\n",
       " 'scans': 1,\n",
       " 'data': 1,\n",
       " 'certain': 1,\n",
       " 'artificial': 1,\n",
       " 'challenge': 1,\n",
       " 'physical': 1,\n",
       " '2027': 1,\n",
       " 'procedures': 1,\n",
       " 'job': 1,\n",
       " 'respondents': 1,\n",
       " 'robots': 1,\n",
       " 'learning': 1,\n",
       " 'sas': 1,\n",
       " 'system': 1,\n",
       " 'question': 1,\n",
       " 'situations': 1,\n",
       " 'structure': 1,\n",
       " 'calculations': 1,\n",
       " 'industry': 1,\n",
       " 'replace': 1,\n",
       " 'banking': 1,\n",
       " 'organizations': 1,\n",
       " 'dji': 1,\n",
       " 'apparent': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'input.txt'\n",
    "with open(path, encoding='utf8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "data\n",
    "\n",
    "#a\n",
    "data = data.split()\n",
    "words_fre_a = {}\n",
    "\n",
    "for word in data:\n",
    "    if word in words_fre_a:\n",
    "        words_fre_a[word] += 1\n",
    "    else:\n",
    "        words_fre_a[word] = 1\n",
    "\n",
    "words_fre_a\n",
    "\n",
    "#b\n",
    "import nltk\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.add('would')\n",
    "stop_words.add('could')\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    new_data = file.read().lower()\n",
    "\n",
    "special_letters = {'.', ',', ':', '-', '!', '*', '(', ')', '-', ';', 'I', 'V', \"'\", '’', '‘', '?', '\\n'\n",
    "                   , '/', '#', '$', '^', '@', '%', '{', '}', '[', ']', '(', ')', '|', '~', '=', '_'}\n",
    "for letter in special_letters:\n",
    "    new_data = new_data.replace(letter, ' ')\n",
    "    words = new_data.split()\n",
    "    wordword = set(words) - stop_words\n",
    "\n",
    "words_fre_b = {}\n",
    "\n",
    "#c\n",
    "for word in wordword:\n",
    "    if word in words_fre_b:\n",
    "        words_fre_b[word] += 1\n",
    "    else:\n",
    "        words_fre_b[word] = 1\n",
    "\n",
    "words_fre_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
