{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friends 7\n"
     ]
    }
   ],
   "source": [
    "sentence = input()\n",
    "\n",
    "sentence = sentence.split()\n",
    "\n",
    "max = 0\n",
    "longest = \"\"\n",
    "for words in sentence:\n",
    "    count = 0\n",
    "    for letters in words:\n",
    "        count += 1\n",
    "    if max < count:\n",
    "        max = count\n",
    "        longest = words\n",
    "\n",
    "print(longest, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NASA'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = input()\n",
    "\n",
    "sentence = sentence.split()\n",
    "\n",
    "result = \"\"\n",
    "for words in sentence:\n",
    "    if words[0].isupper():\n",
    "        result += words[0]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello-friends'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = input()\n",
    "n = int(input())\n",
    "\n",
    "sentence = sentence.split()\n",
    "\n",
    "result = \"\"\n",
    "for words in sentence:\n",
    "    count = 0\n",
    "    for letters in words:\n",
    "        count += 1\n",
    "    if count >= n:\n",
    "        result += words + \"-\"\n",
    "\n",
    "result = result[:-1]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artificial': 22,\n",
       " 'intelligence': 24,\n",
       " 'ai': 23,\n",
       " 'technology': 5,\n",
       " 'allow': 4,\n",
       " 'computer': 12,\n",
       " 'machine': 5,\n",
       " 'simulate': 4,\n",
       " 'human': 9,\n",
       " 'problem': 2,\n",
       " 'solve': 2,\n",
       " 'task': 2,\n",
       " 'ideal': 1,\n",
       " 'characteristic': 1,\n",
       " 'ability': 2,\n",
       " 'rationalize': 1,\n",
       " 'take': 1,\n",
       " 'action': 1,\n",
       " 'achieve': 1,\n",
       " 'specific': 1,\n",
       " 'goal': 1,\n",
       " 'research': 1,\n",
       " 'begin': 1,\n",
       " '1950s': 1,\n",
       " 'use': 8,\n",
       " '1960s': 1,\n",
       " 'unite': 1,\n",
       " 'state': 1,\n",
       " 'department': 1,\n",
       " 'defense': 1,\n",
       " 'train': 2,\n",
       " 'mimic': 1,\n",
       " 'reason': 2,\n",
       " 'sa': 1,\n",
       " 'subset': 1,\n",
       " 'learn': 4,\n",
       " 'ml': 1,\n",
       " 'concept': 1,\n",
       " 'program': 4,\n",
       " 'automatically': 1,\n",
       " 'adapt': 2,\n",
       " 'new': 1,\n",
       " 'data': 2,\n",
       " 'without': 1,\n",
       " 'assistance': 1,\n",
       " 'key': 1,\n",
       " 'takeaway': 1,\n",
       " 'capability': 1,\n",
       " 'algorithm': 5,\n",
       " 'part': 2,\n",
       " 'structure': 2,\n",
       " 'simple': 4,\n",
       " 'application': 8,\n",
       " 'complex': 3,\n",
       " 'one': 3,\n",
       " 'help': 4,\n",
       " 'frame': 2,\n",
       " 'strong': 3,\n",
       " 'apparent': 1,\n",
       " 'play': 4,\n",
       " 'chess': 3,\n",
       " 'self': 4,\n",
       " 'drive': 4,\n",
       " 'car': 5,\n",
       " 'bank': 2,\n",
       " 'system': 5,\n",
       " 'detect': 2,\n",
       " 'fraudulent': 2,\n",
       " 'activity': 2,\n",
       " 'client': 1,\n",
       " 'financial': 2,\n",
       " 'advisor': 1,\n",
       " 'discussion': 1,\n",
       " 'guide': 2,\n",
       " 'invest': 1,\n",
       " 'download': 1,\n",
       " 'work': 1,\n",
       " 'commonly': 1,\n",
       " 'bring': 1,\n",
       " 'mind': 1,\n",
       " 'implementation': 1,\n",
       " 'robot': 1,\n",
       " 'evolve': 1,\n",
       " 'previous': 1,\n",
       " 'benchmark': 1,\n",
       " 'define': 1,\n",
       " 'become': 2,\n",
       " 'outdated': 1,\n",
       " 'enable': 2,\n",
       " 'include': 5,\n",
       " 'vision': 1,\n",
       " 'identify': 3,\n",
       " 'object': 2,\n",
       " 'people': 2,\n",
       " 'picture': 1,\n",
       " 'photo': 1,\n",
       " 'natural': 1,\n",
       " 'language': 2,\n",
       " 'process': 2,\n",
       " 'nlp': 1,\n",
       " 'understand': 1,\n",
       " 'graphical': 1,\n",
       " 'unit': 1,\n",
       " 'chip': 1,\n",
       " 'form': 1,\n",
       " 'graphic': 1,\n",
       " 'image': 2,\n",
       " 'mathematical': 1,\n",
       " 'calculation': 1,\n",
       " 'internet': 1,\n",
       " 'thing': 1,\n",
       " 'network': 2,\n",
       " 'physical': 1,\n",
       " 'device': 1,\n",
       " 'vehicle': 1,\n",
       " 'embed': 1,\n",
       " 'sensor': 1,\n",
       " 'software': 1,\n",
       " 'connectivity': 1,\n",
       " 'collect': 1,\n",
       " 'share': 2,\n",
       " 'two': 1,\n",
       " 'component': 1,\n",
       " 'communicate': 1,\n",
       " 'often': 1,\n",
       " 'type': 4,\n",
       " 'narrow': 2,\n",
       " 'also': 1,\n",
       " 'know': 1,\n",
       " 'weak': 2,\n",
       " 'design': 1,\n",
       " 'carry': 2,\n",
       " 'particular': 1,\n",
       " 'job': 2,\n",
       " 'video': 1,\n",
       " 'game': 2,\n",
       " 'like': 4,\n",
       " 'personal': 1,\n",
       " 'assistant': 2,\n",
       " 'amazon': 1,\n",
       " 'alexa': 1,\n",
       " 'apple': 1,\n",
       " 'siri': 1,\n",
       " 'user': 1,\n",
       " 'ask': 1,\n",
       " 'question': 1,\n",
       " 'answer': 1,\n",
       " 'general': 1,\n",
       " 'consider': 1,\n",
       " 'tend': 2,\n",
       " 'complicate': 1,\n",
       " 'find': 1,\n",
       " 'hospital': 1,\n",
       " 'operate': 2,\n",
       " 'room': 2,\n",
       " 'super': 2,\n",
       " 'strictly': 1,\n",
       " 'theoretical': 1,\n",
       " 'yet': 1,\n",
       " 'realize': 1,\n",
       " 'think': 1,\n",
       " 'possess': 1,\n",
       " 'cognitive': 1,\n",
       " 'surpass': 1,\n",
       " 'being': 1,\n",
       " 'apply': 1,\n",
       " 'many': 3,\n",
       " 'sector': 1,\n",
       " 'industry': 5,\n",
       " 'healthcare': 2,\n",
       " 'suggest': 1,\n",
       " 'drug': 1,\n",
       " 'dosage': 1,\n",
       " 'treatment': 1,\n",
       " 'aid': 1,\n",
       " 'surgical': 1,\n",
       " 'procedure': 1,\n",
       " 'example': 2,\n",
       " 'flag': 1,\n",
       " 'streamline': 1,\n",
       " 'make': 2,\n",
       " 'trade': 3,\n",
       " 'easy': 1,\n",
       " '2022': 1,\n",
       " 'enter': 1,\n",
       " 'mainstream': 1,\n",
       " 'generative': 2,\n",
       " 'pre': 1,\n",
       " 'transformer': 1,\n",
       " 'popular': 1,\n",
       " 'openai': 1,\n",
       " 'dall': 1,\n",
       " 'e': 1,\n",
       " 'text': 1,\n",
       " 'tool': 1,\n",
       " 'chatgpt': 1,\n",
       " 'accord': 1,\n",
       " '2024': 1,\n",
       " 'survey': 1,\n",
       " 'deloitte': 1,\n",
       " '79': 1,\n",
       " 'respondent': 1,\n",
       " 'leader': 1,\n",
       " 'expect': 1,\n",
       " 'transform': 1,\n",
       " 'organization': 1,\n",
       " '2027': 1,\n",
       " 'reactive': 3,\n",
       " 'optimize': 2,\n",
       " 'output': 1,\n",
       " 'base': 1,\n",
       " 'set': 1,\n",
       " 'input': 1,\n",
       " 'best': 1,\n",
       " 'strategy': 1,\n",
       " 'win': 1,\n",
       " 'fairly': 1,\n",
       " 'static': 1,\n",
       " 'unable': 1,\n",
       " 'novel': 1,\n",
       " 'situation': 1,\n",
       " 'concern': 2,\n",
       " 'may': 3,\n",
       " 'affect': 1,\n",
       " 'employment': 1,\n",
       " 'look': 1,\n",
       " 'automate': 1,\n",
       " 'certain': 1,\n",
       " 'intelligent': 1,\n",
       " 'machinery': 1,\n",
       " 'employee': 1,\n",
       " 'push': 1,\n",
       " 'workforce': 1,\n",
       " 'remove': 1,\n",
       " 'need': 1,\n",
       " 'taxi': 1,\n",
       " 'manufacturer': 1,\n",
       " 'easily': 1,\n",
       " 'replace': 1,\n",
       " 'labor': 1,\n",
       " 'skill': 1,\n",
       " 'obsolete': 1,\n",
       " 'setting': 1,\n",
       " 'assist': 1,\n",
       " 'diagnostics': 1,\n",
       " 'small': 1,\n",
       " 'anomaly': 1,\n",
       " 'scan': 1,\n",
       " 'good': 1,\n",
       " 'triangulate': 1,\n",
       " 'diagnose': 1,\n",
       " 'patient': 2,\n",
       " 'symptom': 1,\n",
       " 'vitals': 1,\n",
       " 'classify': 1,\n",
       " 'maintain': 1,\n",
       " 'track': 1,\n",
       " 'medical': 1,\n",
       " 'record': 1,\n",
       " 'deal': 1,\n",
       " 'health': 1,\n",
       " 'insurance': 1,\n",
       " 'claim': 1,\n",
       " 'diversify': 1,\n",
       " 'portfolio': 1,\n",
       " 'index': 2,\n",
       " 'imagine': 1,\n",
       " 'possibility': 1,\n",
       " 'fund': 2,\n",
       " 'join': 1,\n",
       " 'vantage': 2,\n",
       " 'elite': 2,\n",
       " 'challenge': 1,\n",
       " 'major': 1,\n",
       " 'p': 1,\n",
       " '500': 1,\n",
       " 'nasdaq': 1,\n",
       " 'dji': 1,\n",
       " 'cfds': 1,\n",
       " '200': 1,\n",
       " '000': 1,\n",
       " 'earn': 1,\n",
       " '80': 1,\n",
       " 'profit': 1,\n",
       " 'upon': 1,\n",
       " 'successfully': 1,\n",
       " 'trader': 1}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a\n",
    "path = 'input.txt'\n",
    "with open(path, encoding='utf8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "words_fre_a = {}\n",
    "\n",
    "for word in data:\n",
    "    if word in words_fre_a:\n",
    "        words_fre_a[word] += 1\n",
    "    else:\n",
    "        words_fre_a[word] = 1\n",
    "\n",
    "#b\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')  # To tokenize the text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.add('would')\n",
    "stop_words.add('could')\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    new_data = f.read().lower() #lowercase\n",
    "\n",
    "special_letters = {'.', ',', ':', '-', '!', '*', '(', ')', '-', ';', 'I', 'V', \"'\", '’', '‘',\n",
    "                   '@', '#', '$', '%', '^', '&', '{', '}', '[', ']', '~', '|', '\\\\', '=', '_',\n",
    "                   '\\\"', '?', '/', '+', '\\n'}\n",
    "for letter in special_letters:\n",
    "    new_data = new_data.replace(letter, ' ') #remove punctuations, special symbols, spaces and \\n\n",
    "\n",
    "filtered_words = [word for word in words if word not in stop_words] #remove stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_verb = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in filtered_words] \n",
    "lemma_noun = [lemmatizer.lemmatize(word, pos=wordnet.NOUN) for word in lemma_verb]\n",
    "lemma_adj = [lemmatizer.lemmatize(word, pos=wordnet.ADJ) for word in lemma_noun]\n",
    "lemma_adv = [lemmatizer.lemmatize(word, pos=wordnet.ADV) for word in lemma_adj]\n",
    "\n",
    "#c\n",
    "words_fre_b ={}\n",
    "for word in lemma_adv:\n",
    "    if word in words_fre_b:\n",
    "        words_fre_b[word] += 1\n",
    "    else:\n",
    "        words_fre_b[word] = 1\n",
    "\n",
    "words_fre_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
